{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/progyan.das/models/candleGP.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.0.62.168/home/progyan.das/models/candleGP.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.0.62.168/home/progyan.das/models/candleGP.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m parameter\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.0.62.168/home/progyan.das/models/candleGP.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mMeanFunction\u001b[39;00m(torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.0.62.168/home/progyan.das/models/candleGP.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.0.62.168/home/progyan.das/models/candleGP.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m    The base mean function class.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.0.62.168/home/progyan.das/models/candleGP.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m    To implement a mean function, write the __call__ method. This takes a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.0.62.168/home/progyan.das/models/candleGP.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m    example.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.0.62.168/home/progyan.das/models/candleGP.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from . import parameter\n",
    "\n",
    "class MeanFunction(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The base mean function class.\n",
    "    To implement a mean function, write the __call__ method. This takes a\n",
    "    tensor X and returns a tensor m(X). In accordance with the GPflow\n",
    "    standard, each row of X represents one datum, and each row of Y is computed\n",
    "    independently for each row of X.\n",
    "    MeanFunction classes can have parameters, see the Linear class for an\n",
    "    example.\n",
    "    \"\"\"\n",
    "    def forward(self, X):\n",
    "        raise NotImplementedError(\"Implement the forward method for this mean function\")\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return MeanAdditive(self, other)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return MeanProduct(self, other)\n",
    "\n",
    "\n",
    "class Zero(MeanFunction):\n",
    "    def forward(self, X):\n",
    "        return torch.zeros(X.size(0), 1, dtype=X.dtype, device=X.device)\n",
    "\n",
    "class Linear(MeanFunction):\n",
    "    \"\"\"\n",
    "    y_i = A x_i + b\n",
    "    \"\"\"\n",
    "    def __init__(self, A=None, b=None):\n",
    "        \"\"\"\n",
    "        A is a matrix which maps each element of X to Y, b is an additive\n",
    "        constant.\n",
    "        If X has N rows and D columns, and Y is intended to have Q columns,\n",
    "        then A must be D x Q, b must be a vector of length Q.\n",
    "        \"\"\"\n",
    "        A = torch.ones((1, 1)) if A is None else A\n",
    "        b = torch.zeros(1) if b is None else b\n",
    "        MeanFunction.__init__(self)\n",
    "        if A.dim()==1:\n",
    "            A = A.unsqueeze(1)\n",
    "        self.A = parameter.Param(A)\n",
    "        self.b = parameter.Param(b)\n",
    "\n",
    "    def __call__(self, X):\n",
    "        return torch.matmul(X, self.A.get()) + self.b.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy\n",
    "import abc\n",
    "import torch\n",
    "\n",
    "class ParamWithPrior(torch.nn.Parameter):\n",
    "    @abc.abstractmethod\n",
    "    def get(self):\n",
    "        pass\n",
    "    @abc.abstractmethod\n",
    "    def log_jacobian_tensor(self):\n",
    "        pass\n",
    "    @abc.abstractstaticmethod\n",
    "    def untransform(t, out=None):\n",
    "        pass\n",
    "    def __init__(self, val, prior=None, dtype=torch.float32):\n",
    "        pass\n",
    "    def __new__(cls, val, prior=None, dtype=torch.float32):\n",
    "        if numpy.isscalar(val):\n",
    "            val = torch.tensor([val], dtype=dtype)\n",
    "        raw = cls.untransform(val)\n",
    "        obj = super(ParamWithPrior, cls).__new__(cls, raw)\n",
    "        obj.prior = prior\n",
    "        return obj\n",
    "    def set(self, t):\n",
    "        if numpy.isscalar(t):\n",
    "            t = torch.tensor(t, dtype=self.dtype)\n",
    "        self.untransform(t, out=self)\n",
    "    def get_prior(self):\n",
    "        if self.prior is None:\n",
    "            return 0.0\n",
    "        \n",
    "        log_jacobian = self.log_jacobian() #(unconstrained_tensor)\n",
    "        logp_var = self.prior.logp(self.get())\n",
    "        return log_jacobian+logp_var\n",
    "\n",
    "class PositiveParam(ParamWithPrior): # log(1+exp(r))\n",
    "    @staticmethod\n",
    "    def untransform(t, out=None):\n",
    "        with torch.no_grad():\n",
    "            return torch.log(torch.exp(t) - 1, out=out)\n",
    "    def get(self):\n",
    "        return torch.log(1 + torch.exp(self))\n",
    "    def log_jacobian(self):\n",
    "        return -(torch.nn.functional.softplus(-self))\n",
    "\n",
    "class Param(ParamWithPrior): # unconstrained / untransformed\n",
    "    @staticmethod\n",
    "    def untransform(t, out=None):\n",
    "        if out is None:\n",
    "            return t\n",
    "        else:\n",
    "            return out.copy_(t)\n",
    "    def get(self):\n",
    "        return self\n",
    "    def log_jacobian(self):\n",
    "        return torch.zeros((), dtype=self.dtype)  # dimension?\n",
    "\n",
    "\n",
    "class LowerTriangularParam(ParamWithPrior):\n",
    "    \"\"\"\n",
    "    A transform of the form\n",
    "       tri_mat = vec_to_tri(x)\n",
    "    x is a free variable, y is always a list of lower triangular matrices sized\n",
    "    (N x N x D).\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def untransform(t, out=None):\n",
    "        ii,jj = numpy.tril_indices(t.size(0))\n",
    "        return t[ii,jj]\n",
    "    def get(self):\n",
    "        numel = self.size(0)\n",
    "        N = int((2*numel+0.25)**0.5-0.5)\n",
    "        ii,jj = numpy.tril_indices(N)\n",
    "        if self.dim()==2:\n",
    "            mat = torch.zeros(N,N, self.size(1), dtype=self.dtype)\n",
    "        else:\n",
    "            mat = torch.zeros(N, N, dtype=self.dtype)\n",
    "        mat[ii,jj] = self\n",
    "        return mat\n",
    "    def log_jacobian(self):\n",
    "        return torch.zeros((), dtype=self.dtype)  # dimension?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2016 James Hensman, Mark van der Wilk, Valentine Svensson, alexggmatthews, fujiisoup\n",
    "# Copyright 2017 Thomas Viehmann\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# import abc\n",
    "import torch\n",
    "import numpy\n",
    "\n",
    "\n",
    "\n",
    "class GPModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A base class for Gaussian process models, that is, those of the form\n",
    "    .. math::\n",
    "       :nowrap:\n",
    "       \\\\begin{align}\n",
    "       \\\\theta & \\sim p(\\\\theta) \\\\\\\\\n",
    "       f       & \\sim \\\\mathcal{GP}(m(x), k(x, x'; \\\\theta)) \\\\\\\\\n",
    "       f_i       & = f(x_i) \\\\\\\\\n",
    "       y_i\\,|\\,f_i     & \\sim p(y_i|f_i)\n",
    "       \\\\end{align}\n",
    "    This class mostly adds functionality to compile predictions. To use it,\n",
    "    inheriting classes must define a build_predict function, which computes\n",
    "    the means and variances of the latent function. This gets compiled\n",
    "    similarly to build_likelihood in the Model class.\n",
    "    These predictions are then pushed through the likelihood to obtain means\n",
    "    and variances of held out data, self.predict_y.\n",
    "    The predictions can also be used to compute the (log) density of held-out\n",
    "    data via self.predict_density.\n",
    "    For handling another data (Xnew, Ynew), set the new value to self.X and self.Y\n",
    "    >>> m.X = Xnew\n",
    "    >>> m.Y = Ynew\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, Y, kern, likelihood, mean_function, name=None, jitter_level=1e-6):\n",
    "        super(GPModel, self).__init__()\n",
    "        self.name = name\n",
    "        self.mean_function = mean_function or mean_functions.Zero()\n",
    "        self.kern = kern\n",
    "        self.likelihood = likelihood\n",
    "        self.jitter_level = jitter_level\n",
    "\n",
    "        if isinstance(X, numpy.ndarray):\n",
    "            # X is a data matrix; each row represents one instance\n",
    "            X = torch.from_numpy(X)\n",
    "        if isinstance(Y, numpy.ndarray):\n",
    "            # Y is a data matrix, rows correspond to the rows in X,\n",
    "            # columns are treated independently\n",
    "            Y = torch.from_numpy(Y)\n",
    "        self.X, self.Y = X, Y\n",
    "\n",
    "    # @abc.abstractmethod\n",
    "    def compute_log_prior(self):\n",
    "        \"\"\"Compute the log prior of the model.\"\"\"\n",
    "        pass\n",
    "\n",
    "    # @abc.abstractmethod\n",
    "    def compute_log_likelihood(self, X=None, Y=None):\n",
    "        \"\"\"Compute the log likelihood of the model.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def objective(self, X=None, Y=None):\n",
    "        pos_objective = self.compute_log_likelihood(X, Y)\n",
    "        for param in self.parameters():\n",
    "            if isinstance(param, parameter.ParamWithPrior):\n",
    "                pos_objective = pos_objective + param.get_prior()\n",
    "        return -pos_objective\n",
    "\n",
    "    def forward(self, X=None, Y=None):\n",
    "        return self.objective(X, Y)\n",
    "\n",
    "    # @abc.abstractmethod\n",
    "    def predict_f(self, Xnew, full_cov=False):\n",
    "        \"\"\"\n",
    "        Compute the mean and variance of the latent function(s) at the points\n",
    "        Xnew.\n",
    "        \"\"\"\n",
    "        pass\n",
    "        #return self._build_predict(Xnew) #    @autoflow((settings.tf_float, [None, None]))\n",
    "\n",
    "    def predict_f_full_cov(self, Xnew):\n",
    "        \"\"\"\n",
    "        Compute the mean and covariance matrix of the latent function(s) at the\n",
    "        points Xnew.\n",
    "        \"\"\"\n",
    "        return self.predict_f(Xnew, full_cov=True)\n",
    "\n",
    "    def predict_f_samples(self, Xnew, num_samples):\n",
    "        \"\"\"\n",
    "        Produce samples from the posterior latent function(s) at the points\n",
    "        Xnew.\n",
    "        \"\"\"\n",
    "        mu, var = self.predict_f(Xnew, full_cov=True)\n",
    "        jitter = torch.eye(mu.size(0), dtype=mu.dtype,\n",
    "                           device=mu.device) * self.jitter_level\n",
    "        samples = []\n",
    "        for i in range(self.num_latent):  # TV-Todo: batch??\n",
    "            L = torch.cholesky(var[:, :, i] + jitter, upper=False)\n",
    "            V = torch.randn(L.size(0), num_samples,\n",
    "                            dtype=L.dtype, device=L.device)\n",
    "            samples.append(mu[:, i:i + 1] + torch.matmul(L, V))\n",
    "        return torch.stack(samples, dim=0)  # TV-Todo: transpose?\n",
    "\n",
    "    def predict_y(self, Xnew):\n",
    "        \"\"\"\n",
    "        Compute the mean and variance of held-out data at the points Xnew\n",
    "        \"\"\"\n",
    "        pred_f_mean, pred_f_var = self.predict_f(Xnew)\n",
    "        return self.likelihood.predict_mean_and_var(pred_f_mean, pred_f_var)\n",
    "\n",
    "    def predict_density(self, Xnew, Ynew):\n",
    "        \"\"\"\n",
    "        Compute the (log) density of the data Ynew at the points Xnew\n",
    "        Note that this computes the log density of the data individually,\n",
    "        ignoring correlations between them. The result is a matrix the same\n",
    "        shape as Ynew containing the log densities.\n",
    "        \"\"\"\n",
    "        pred_f_mean, pred_f_var = self.predict_f(Xnew)\n",
    "        return self.likelihood.predict_density(pred_f_mean, pred_f_var, Ynew)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2016 Valentine Svensson, James Hensman, alexggmatthews\n",
    "# Copyright 2017 Thomas Viehmann\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy\n",
    "\n",
    "\n",
    "def batch_tril(A):\n",
    "    B = A.clone()\n",
    "    ii, jj = numpy.triu_indices(B.size(-2), k=1, m=B.size(-1))\n",
    "    B[..., ii, jj] = 0\n",
    "    return B\n",
    "\n",
    "\n",
    "def batch_diag(A):\n",
    "    ii, jj = numpy.diag_indices(min(A.size(-2), A.size(-1)))\n",
    "    return A[..., ii, jj]\n",
    "\n",
    "\n",
    "def conditional(Xnew, X, kern, f, full_cov=False, q_sqrt=None, whiten=False, jitter_level=1e-6):\n",
    "    \"\"\"\n",
    "    Given F, representing the GP at the points X, produce the mean and\n",
    "    (co-)variance of the GP at the points Xnew.\n",
    "\n",
    "    Additionally, there may be Gaussian uncertainty about F as represented by\n",
    "    q_sqrt. In this case `f` represents the mean of the distribution and\n",
    "    q_sqrt the square-root of the covariance.\n",
    "\n",
    "    Additionally, the GP may have been centered (whitened) so that\n",
    "        p(v) = N( 0, I)\n",
    "        f = L v\n",
    "    thus\n",
    "        p(f) = N(0, LL^T) = N(0, K).\n",
    "    In this case 'f' represents the values taken by v.\n",
    "\n",
    "    The method can either return the diagonals of the covariance matrix for\n",
    "    each output of the full covariance matrix (full_cov).\n",
    "\n",
    "    We assume K independent GPs, represented by the columns of f (and the\n",
    "    last dimension of q_sqrt).\n",
    "\n",
    "    :param Xnew: data matrix, size N x D.\n",
    "    :param X: data points, size M x D.\n",
    "    :param kern: GPflow kernel.\n",
    "    :param f: data matrix, M x K, representing the function values at X,\n",
    "        for K functions.\n",
    "    :param q_sqrt: matrix of standard-deviations or Cholesky matrices,\n",
    "        size M x K or M x M x K.\n",
    "    :param whiten: boolean of whether to whiten the representation as\n",
    "        described above.\n",
    "\n",
    "    :return: two element tuple with conditional mean and variance.\n",
    "    \"\"\"\n",
    "\n",
    "    # compute kernel stuff\n",
    "    num_data = X.size(0)  # M\n",
    "    num_func = f.size(1)  # K\n",
    "    Kmn = kern.K(X, Xnew)\n",
    "    Kmm = kern.K(X) + torch.eye(num_data, dtype=X.dtype,\n",
    "                                device=X.device) * jitter_level\n",
    "    Lm = torch.cholesky(Kmm, upper=False)\n",
    "\n",
    "    # Compute the projection matrix A\n",
    "    A, _ = torch.solve(Kmn, Lm)\n",
    "\n",
    "    # compute the covariance due to the conditioning\n",
    "    if full_cov:\n",
    "        fvar = kern.K(Xnew) - torch.matmul(A.t(), A)\n",
    "        fvar = fvar.unsqueeze(0).expand(num_func, -1, -1)  # K x N x N\n",
    "    else:\n",
    "        fvar = kern.Kdiag(Xnew) - (A**2).sum(0)\n",
    "        fvar = fvar.unsqueeze(0).expand(num_func, -1)  # K x N\n",
    "    # fvar is K x N x N or K x N\n",
    "\n",
    "    # another backsubstitution in the unwhitened case (complete the inverse of the cholesky decomposition)\n",
    "    if not whiten:\n",
    "        A, _ = torch.solve(A, Lm.t())\n",
    "\n",
    "    # construct the conditional mean\n",
    "    fmean = torch.matmul(A.t(), f)\n",
    "\n",
    "    if q_sqrt is not None:\n",
    "        if q_sqrt.dim() == 2:\n",
    "            LTA = A * q_sqrt.t().unsqueeze(2)  # K x M x N\n",
    "        elif q_sqrt.dim() == 3:\n",
    "            L = batch_tril(q_sqrt.permute(2, 0, 1))  # K x M x M\n",
    "            # A_tiled = tf.tile(tf.expand_dims(A, 0), tf.stack([num_func, 1, 1])) # I don't think I need this\n",
    "            LTA = torch.matmul(L.transpose(-2, -1), A)  # K x M x N\n",
    "        else:  # pragma: no cover\n",
    "            raise ValueError(\n",
    "                \"Bad dimension for q_sqrt :{}\".format(q_sqrt.dim()))\n",
    "        if full_cov:\n",
    "            fvar = fvar + torch.matmul(LTA.t(), LTA)  # K x N x N\n",
    "        else:\n",
    "            fvar = fvar + (LTA**2).sum(1)  # K x N\n",
    "    fvar = fvar.permute(*range(fvar.dim()-1, -1, -1))  # N x K or N x N x K\n",
    "\n",
    "    return fmean, fvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2016 James Hensman, alexggmatthews\n",
    "# Copyright 2017 Thomas Viehmann\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy\n",
    "\n",
    "\n",
    "def batch_tril(A):\n",
    "    B = A.clone()\n",
    "    ii, jj = numpy.triu_indices(B.size(-2), k=1, m=B.size(-1))\n",
    "    B[..., ii, jj] = 0\n",
    "    return B\n",
    "\n",
    "\n",
    "def batch_diag(A):\n",
    "    ii, jj = numpy.diag_indices(min(A.size(-2), A.size(-1)))\n",
    "    return A[..., ii, jj]\n",
    "\n",
    "\n",
    "def gauss_kl_white(q_mu, q_sqrt):\n",
    "    \"\"\"\n",
    "    Compute the KL divergence from\n",
    "\n",
    "          q(x) = N(q_mu, q_sqrt^2)\n",
    "    to\n",
    "          p(x) = N(0, I)\n",
    "\n",
    "    We assume multiple independent distributions, given by the columns of\n",
    "    q_mu and the last dimension of q_sqrt.\n",
    "\n",
    "    q_mu is a matrix, each column contains a mean\n",
    "\n",
    "    q_sqrt is a 3D tensor, each matrix within is a lower triangular square-root\n",
    "        matrix of the covariance.\n",
    "    \"\"\"\n",
    "    KL = 0.5 * (q_mu**2).sum()               # Mahalanobis term\n",
    "    KL += -0.5 * numpy.prod(q_sqrt.size()[1:])  # constant term\n",
    "    L = batch_tril(q_sqrt.permute(2, 0, 1))   # force lower triangle\n",
    "    KL -= batch_diag(L).log().sum()          # logdet sum log(L**2)\n",
    "    KL += 0.5 * (L**2).sum()                 # Trace term.\n",
    "    return KL\n",
    "\n",
    "\n",
    "def gauss_kl_white_diag(q_mu, q_sqrt):\n",
    "    \"\"\"\n",
    "    Compute the KL divergence from\n",
    "\n",
    "          q(x) = N(q_mu, q_sqrt^2)\n",
    "    to\n",
    "          p(x) = N(0, I)\n",
    "\n",
    "    We assume multiple independent distributions, given by the columns of\n",
    "    q_mu and q_sqrt\n",
    "\n",
    "    q_mu is a matrix, each column contains a mean\n",
    "\n",
    "    q_sqrt is a matrix, each column represents the diagonal of a square-root\n",
    "        matrix of the covariance.\n",
    "    \"\"\"\n",
    "\n",
    "    KL = 0.5 * (q_mu**2).sum()                    # Mahalanobis term\n",
    "    KL += -0.5 * q_sqrt.numel()\n",
    "    KL = KL - q_sqrt.abs().log().sum()            # Log-det of q-cov\n",
    "    KL += 0.5 * (q_sqrt**2).sum()                 # Trace term\n",
    "    return KL\n",
    "\n",
    "\n",
    "def gauss_kl_diag(q_mu, q_sqrt, K):\n",
    "    \"\"\"\n",
    "    Compute the KL divergence from\n",
    "\n",
    "          q(x) = N(q_mu, q_sqrt^2)\n",
    "    to\n",
    "          p(x) = N(0, K)\n",
    "\n",
    "    We assume multiple independent distributions, given by the columns of\n",
    "    q_mu and q_sqrt.\n",
    "\n",
    "    q_mu is a matrix, each column contains a mean\n",
    "\n",
    "    q_sqrt is a matrix, each column represents the diagonal of a square-root\n",
    "        matrix of the covariance of q.\n",
    "\n",
    "    K is a positive definite matrix: the covariance of p.\n",
    "    \"\"\"\n",
    "    L = torch.potrf(K, upper=False)\n",
    "    alpha, _ = torch.gesv(q_mu, L)\n",
    "    KL = 0.5 * (alpha**2).sum()                  # Mahalanobis term.\n",
    "    num_latent = q_sqrt.size(1)\n",
    "    KL += num_latent * torch.diag(L).log().sum()  # Prior log-det term.\n",
    "    KL += -0.5 * q_sqrt.numel()                  # constant term\n",
    "    KL += - q_sqrt.log().sum()                   # Log-det of q-cov\n",
    "    K_inv, _ = torch.cholesky(\n",
    "        torch.eye(L.size(0), dtype=L.dtype, device=L.device), L, upper=False)\n",
    "    # Trace term.\n",
    "    KL += 0.5 * (torch.diag(K_inv).unsqueeze(1) * q_sqrt**2).sum()\n",
    "    return KL\n",
    "\n",
    "\n",
    "def gauss_kl(q_mu, q_sqrt, K):\n",
    "    \"\"\"\n",
    "    Compute the KL divergence from\n",
    "\n",
    "          q(x) = N(q_mu, q_sqrt^2)\n",
    "    to\n",
    "          p(x) = N(0, K)\n",
    "\n",
    "    We assume multiple independent distributions, given by the columns of\n",
    "    q_mu and the last dimension of q_sqrt.\n",
    "\n",
    "    q_mu is a matrix, each column contains a mean.\n",
    "\n",
    "    q_sqrt is a 3D tensor, each matrix within is a lower triangular square-root\n",
    "        matrix of the covariance of q.\n",
    "\n",
    "    K is a positive definite matrix: the covariance of p.\n",
    "    \"\"\"\n",
    "    L = torch.potrf(K, upper=False)\n",
    "    alpha, _ = torch.gesv(q_mu, L)\n",
    "    KL = 0.5 * (alpha**2).sum()                  # Mahalanobis term.\n",
    "    num_latent = q_sqrt.size(2)\n",
    "    KL += num_latent * torch.tiag(L).log().sum()  # Prior log-det term.\n",
    "    KL += -0.5 * numpy.prod(q_sqrt.size()[1:])  # constant term\n",
    "    Lq = batch_tril(q_sqrt.permute(2, 0, 1))  # force lower triangle\n",
    "    KL += batch_diag(Lq).log().sum()         # logdet\n",
    "    LiLq, _ = torch.gesv(Lq.view(-1, L.size(-1)),\n",
    "                         L).view(*L.size())  # batch with same LHS\n",
    "    KL += 0.5 * (LiLq**2).sum()  # Trace term\n",
    "    return KL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVGP(GPModel):\n",
    "    \"\"\"\n",
    "    This is the Sparse Variational GP (SVGP). The key reference is\n",
    "    ::\n",
    "      @inproceedings{hensman2014scalable,\n",
    "        title={Scalable Variational Gaussian Process Classification},\n",
    "        author={Hensman, James and Matthews,\n",
    "                Alexander G. de G. and Ghahramani, Zoubin},\n",
    "        booktitle={Proceedings of AISTATS},\n",
    "        year={2015}\n",
    "      }\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, Y, kern, likelihood, Z,\n",
    "                 mean_function=None,\n",
    "                 num_latent=None,\n",
    "                 q_diag=False,\n",
    "                 whiten=True,\n",
    "                 minibatch_size=None,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        - X is a data matrix, size N x D\n",
    "        - Y is a data matrix, size N x R\n",
    "        - kern, likelihood, mean_function are appropriate GPflow objects\n",
    "        - Z is a matrix of pseudo inputs, size M x D\n",
    "        - num_latent is the number of latent process to use, default to\n",
    "          Y.shape[1]\n",
    "        - q_diag is a boolean. If True, the covariance is approximated by a\n",
    "          diagonal matrix.\n",
    "        - whiten is a boolean. If True, we use the whitened representation of\n",
    "          the inducing points.\n",
    "        \"\"\"\n",
    "        # sort out the X, Y into MiniBatch objects if required.\n",
    "        self.minibatch_size = minibatch_size\n",
    "\n",
    "        # init the super class, accept args\n",
    "        super(SVGP, self).__init__(X, Y, kern,\n",
    "                                   likelihood, mean_function, **kwargs)\n",
    "        self.num_data = X.size(0)\n",
    "        self.q_diag, self.whiten = q_diag, whiten\n",
    "        self.Z = parameter.Param(Z)\n",
    "        self.num_latent = num_latent or Y.size(1)\n",
    "        self.num_inducing = Z.size(0)\n",
    "\n",
    "        # init variational parameters\n",
    "        self.q_mu = parameter.Param(self.Z.data.new(\n",
    "            self.num_inducing, self.num_latent).zero_())\n",
    "        if self.q_diag:\n",
    "            self.q_sqrt = parameter.PositiveParam(self.Z.data.new(\n",
    "                self.num_inducing, self.num_latent).fill_(1.0))\n",
    "        else:\n",
    "            q_sqrt = torch.eye(self.num_inducing, out=self.Z.data.new()).unsqueeze(\n",
    "                2).expand(-1, -1, self.num_latent)\n",
    "            self.q_sqrt = parameter.LowerTriangularParam(\n",
    "                q_sqrt)  # should the diagonal be all positive?\n",
    "\n",
    "    def prior_KL(self):\n",
    "        if self.whiten:\n",
    "            if self.q_diag:\n",
    "                KL = gauss_kl_white_diag(\n",
    "                    self.q_mu.get(), self.q_sqrt.get())\n",
    "            else:\n",
    "                KL = gauss_kl_white(\n",
    "                    self.q_mu.get(), self.q_sqrt.get())\n",
    "        else:\n",
    "            K = self.kern.K(self.Z.get()) + torch.eye(self.num_inducing,\n",
    "                                                      out=self.Z.new()) * self.jitter_level\n",
    "            if self.q_diag:\n",
    "                KL = gauss_kl_diag(\n",
    "                    self.q_mu.get(), self.q_sqrt.get(), K)\n",
    "            else:\n",
    "                KL = gauss_kl(\n",
    "                    self.q_mu.get(), self.q_sqrt.get(), K)\n",
    "        return KL\n",
    "\n",
    "    def compute_log_likelihood(self, X=None, Y=None):\n",
    "        \"\"\"\n",
    "        This gives a variational bound on the model likelihood.\n",
    "        \"\"\"\n",
    "\n",
    "        if X is None:\n",
    "            X = self.X\n",
    "        if Y is None:\n",
    "            Y = self.Y\n",
    "\n",
    "        # Get prior KL.\n",
    "        KL = self.prior_KL()\n",
    "\n",
    "        # Get conditionals\n",
    "        fmean, fvar = self.predict_f(X, full_cov=False)\n",
    "\n",
    "        # Get variational expectations.\n",
    "        var_exp = self.likelihood.variational_expectations(fmean, fvar, Y)\n",
    "\n",
    "        # re-scale for minibatch size\n",
    "        scale = float(self.num_data) / X.size(0)\n",
    "\n",
    "        return var_exp.sum() * scale - KL\n",
    "\n",
    "    def predict_f(self, Xnew, full_cov=False):\n",
    "        mu, var = conditional(Xnew, self.Z.get(), self.kern, self.q_mu,\n",
    "                                           q_sqrt=self.q_sqrt.get(), full_cov=full_cov, whiten=self.whiten,\n",
    "                                           jitter_level=self.jitter_level)\n",
    "        return mu + self.mean_function(Xnew), var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "%matplotlib inline\n",
    "pyplot.style.use('ggplot')\n",
    "import IPython\n",
    "import sys, os\n",
    "import numpy\n",
    "import time\n",
    "sys.path.append(os.path.join(os.getcwd(),'..'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2016 James Hensman, Valentine Svensson, alexggmatthews\n",
    "# Copyright 2017 Thomas Viehmann\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import torch\n",
    "import numpy\n",
    "\n",
    "class Kern(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The basic kernel class. Handles input_dim and active dims, and provides a\n",
    "    generic '_slice' function to implement them.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, active_dims=None, name=None):\n",
    "        \"\"\"\n",
    "        input dim is an integer\n",
    "        active dims is either an iterable of integers or None.\n",
    "\n",
    "        Input dim is the number of input dimensions to the kernel. If the\n",
    "        kernel is computed on a matrix X which has more columns than input_dim,\n",
    "        then by default, only the first input_dim columns are used. If\n",
    "        different columns are required, then they may be specified by\n",
    "        active_dims.\n",
    "\n",
    "        If active dims is None, it effectively defaults to range(input_dim),\n",
    "        but we store it as a slice for efficiency.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.input_dim = int(input_dim)\n",
    "        if active_dims is None:\n",
    "            self.active_dims = slice(input_dim)\n",
    "        elif isinstance(active_dims, slice):\n",
    "            self.active_dims = active_dims\n",
    "            if active_dims.start is not None and active_dims.stop is not None and active_dims.step is not None:\n",
    "                assert len(range(*active_dims)) == input_dim  # pragma: no cover\n",
    "        else:\n",
    "            self.active_dims = numpy.array(active_dims, dtype=numpy.int32)\n",
    "            assert len(active_dims) == input_dim\n",
    "\n",
    "        self.num_gauss_hermite_points = 20\n",
    "\n",
    "    def eKdiag(self, Xmu, Xcov):\n",
    "        \"\"\"\n",
    "        Computes <K_xx>_q(x).\n",
    "        :param Xmu: Mean (NxD)\n",
    "        :param Xcov: Covariance (NxDxD or NxD)\n",
    "        :return: (N)\n",
    "        \"\"\"\n",
    "        self._check_quadrature()\n",
    "        Xmu, _ = self._slice(Xmu, None)\n",
    "        Xcov = self._slice_cov(Xcov)\n",
    "        return mvnquad(lambda x: self.Kdiag(x, presliced=True),\n",
    "                       Xmu, Xcov,\n",
    "                       self.num_gauss_hermite_points, self.input_dim)  # N\n",
    "\n",
    "    def eKxz(self, Z, Xmu, Xcov):\n",
    "        \"\"\"\n",
    "        Computes <K_xz>_q(x) using quadrature.\n",
    "        :param Z: Fixed inputs (MxD).\n",
    "        :param Xmu: X means (NxD).\n",
    "        :param Xcov: X covariances (NxDxD or NxD).\n",
    "        :return: (NxM)\n",
    "        \"\"\"\n",
    "        self._check_quadrature()\n",
    "        Xmu, Z = self._slice(Xmu, Z)\n",
    "        Xcov = self._slice_cov(Xcov)\n",
    "        M = tf.shape(Z)[0]\n",
    "        return mvnquad(lambda x: self.K(x, Z, presliced=True), Xmu, Xcov, self.num_gauss_hermite_points,\n",
    "                       self.input_dim, Dout=(M,))  # (H**DxNxD, H**D)\n",
    "\n",
    "    def exKxz(self, Z, Xmu, Xcov):\n",
    "        \"\"\"\n",
    "        Computes <x_{t-1} K_{x_t z}>_q(x) for each pair of consecutive X's in\n",
    "        Xmu & Xcov.\n",
    "        :param Z: Fixed inputs (MxD).\n",
    "        :param Xmu: X means (T+1xD).\n",
    "        :param Xcov: 2xT+1xDxD. [0, t, :, :] contains covariances for x_t. [1, t, :, :] contains the cross covariances\n",
    "        for t and t+1.\n",
    "        :return: (TxMxD).\n",
    "        \"\"\"\n",
    "        self._check_quadrature()\n",
    "        # Slicing is NOT needed here. The desired behaviour is to *still* return an NxMxD matrix. As even when the\n",
    "        # kernel does not depend on certain inputs, the output matrix will still contain the outer product between the\n",
    "        # mean of x_{t-1} and K_{x_t Z}. The code here will do this correctly automatically, since the quadrature will\n",
    "        # still be done over the distribution x_{t-1, t}, only now the kernel will not depend on certain inputs.\n",
    "        # However, this does mean that at the time of running this function we need to know the input *size* of Xmu, not\n",
    "        # just `input_dim`.\n",
    "        M = tf.shape(Z)[0]\n",
    "        D = self.input_size if hasattr(self, 'input_size') else self.input_dim  # Number of actual input dimensions\n",
    "        assert Xmu.size(1)==D\n",
    "\n",
    "        # First, transform the compact representation of Xmu and Xcov into a\n",
    "        # list of full distributions.\n",
    "        fXmu = tf.concat((Xmu[:-1, :], Xmu[1:, :]), 1)  # Nx2D\n",
    "        fXcovt = tf.concat((Xcov[0, :-1, :, :], Xcov[1, :-1, :, :]), 2)  # NxDx2D\n",
    "        fXcovb = tf.concat((tf.transpose(Xcov[1, :-1, :, :], (0, 2, 1)), Xcov[0, 1:, :, :]), 2)\n",
    "        fXcov = tf.concat((fXcovt, fXcovb), 1)\n",
    "        return mvnquad(lambda x: self.K(x[:, :D], Z).unsqueeze(2) *\n",
    "                                 x[:, D:].unsqueeze(1),\n",
    "                       fXmu, fXcov, self.num_gauss_hermite_points,\n",
    "                       2 * D, Dout=(M, D))\n",
    "\n",
    "    def eKzxKxz(self, Z, Xmu, Xcov):\n",
    "        \"\"\"\n",
    "        Computes <K_zx Kxz>_q(x).\n",
    "        :param Z: Fixed inputs MxD.\n",
    "        :param Xmu: X means (NxD).\n",
    "        :param Xcov: X covariances (NxDxD or NxD).\n",
    "        :return: NxMxM\n",
    "        \"\"\"\n",
    "        self._check_quadrature()\n",
    "        Xmu, Z = self._slice(Xmu, Z)\n",
    "        Xcov = self._slice_cov(Xcov)\n",
    "        M = Z.size(0)\n",
    "\n",
    "        def KzxKxz(x):\n",
    "            Kxz = self.K(x, Z, presliced=True)\n",
    "            return tf.expand_dims(Kxz, 2) * tf.expand_dims(Kxz, 1)\n",
    "\n",
    "        return mvnquad(KzxKxz,\n",
    "                       Xmu, Xcov, self.num_gauss_hermite_points,\n",
    "                       self.input_dim, Dout=(M, M))\n",
    "\n",
    "    def _check_quadrature(self):\n",
    "        if settings.numerics.ekern_quadrature == \"warn\":\n",
    "            warnings.warn(\"Using numerical quadrature for kernel expectation of %s. Use gpflow.ekernels instead.\" %\n",
    "                          str(type(self)))\n",
    "        if settings.numerics.ekern_quadrature == \"error\" or self.num_gauss_hermite_points == 0:\n",
    "            raise RuntimeError(\"Settings indicate that quadrature may not be used.\")\n",
    "\n",
    "    def _slice(self, X, X2):\n",
    "        \"\"\"\n",
    "        Slice the correct dimensions for use in the kernel, as indicated by\n",
    "        `self.active_dims`.\n",
    "        :param X: Input 1 (NxD).\n",
    "        :param X2: Input 2 (MxD), may be None.\n",
    "        :return: Sliced X, X2, (Nxself.input_dim).\n",
    "        \"\"\"\n",
    "        #if isinstance(self.active_dims, slice):\n",
    "        X = X[:, self.active_dims]\n",
    "        if X2 is not None:\n",
    "            X2 = X2[:, self.active_dims]\n",
    "        # I think advanced indexing does the right thing also for the second case\n",
    "        #else:\n",
    "        assert X.size(1)==self.input_dim\n",
    "        return X, X2\n",
    "\n",
    "    def _slice_cov(self, cov):\n",
    "        \"\"\"\n",
    "        Slice the correct dimensions for use in the kernel, as indicated by\n",
    "        `self.active_dims` for covariance matrices. This requires slicing the\n",
    "        rows *and* columns. This will also turn flattened diagonal\n",
    "        matrices into a tensor of full diagonal matrices.\n",
    "        :param cov: Tensor of covariance matrices (NxDxD or NxD).\n",
    "        :return: N x self.input_dim x self.input_dim.\n",
    "        \"\"\"\n",
    "        cov = tf.cond(tf.equal(tf.rank(cov), 2), lambda: tf.matrix_diag(cov), lambda: cov)\n",
    "\n",
    "        if isinstance(self.active_dims, slice):\n",
    "            cov = cov[..., self.active_dims, self.active_dims]\n",
    "        else:\n",
    "            cov_shape = cov.size()\n",
    "            covr = cov.view(-1, cov_shape[-1], cov_shape[-1])\n",
    "            gather1 = torch.gather(tf.transpose(covr, [2, 1, 0]), self.active_dims)\n",
    "            gather2 = torch.gather(tf.transpose(gather1, [1, 0, 2]), self.active_dims)\n",
    "            cov = tf.reshape(tf.transpose(gather2, [2, 0, 1]),\n",
    "                             tf.concat([cov_shape[:-2], [len(self.active_dims), len(self.active_dims)]], 0))\n",
    "        return cov\n",
    "\n",
    "    def __add__(self, other):\n",
    "        return Add([self, other])\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        return Prod([self, other])\n",
    "\n",
    "\n",
    "class Static(Kern):\n",
    "    \"\"\"\n",
    "    Kernels that do not depend on the value of the inputs are 'Static'.  The only\n",
    "    parameter is a variance.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, variance=1.0, active_dims=None, name=None):\n",
    "        super().__init__(input_dim, active_dims, name=name)\n",
    "        self.variance = parameter.PositiveParam(variance)\n",
    "\n",
    "    def Kdiag(self, X, presliced=False):\n",
    "        return self.variance.get().expand(X.size(0))\n",
    "\n",
    "\n",
    "class Linear(Kern):\n",
    "    def __init__(self, input_dim, variance=1.0, active_dims=None, name=None):\n",
    "        super().__init__(input_dim, active_dims, name=name)\n",
    "        self.variance = parameter.PositiveParam(variance)\n",
    "\n",
    "    def K(self, X, X2=None, presliced=False):\n",
    "        if X2 is None:\n",
    "            X2 = X\n",
    "        return torch.mm(X * self.variance.get(), X2.t())\n",
    "\n",
    "    def Kdiag(self, X, presliced=False):\n",
    "        return ((X**2) * self.variance.get()).sum(1)\n",
    "\n",
    "\n",
    "class White(Static):\n",
    "    \"\"\"\n",
    "    The White kernel\n",
    "    \"\"\"\n",
    "\n",
    "    def K(self, X, X2=None, presliced=False):\n",
    "        if X2 is None:\n",
    "            d = self.variance.get().expand(X.size(0))\n",
    "            return torch.diag(d)\n",
    "        else:\n",
    "            return torch.zeros(X.size(0), X2.size(0), dtype=X.dtype, device=X.device)\n",
    "\n",
    "\n",
    "class Constant(Static):\n",
    "    \"\"\"\n",
    "    The Constant (aka Bias) kernel\n",
    "    \"\"\"\n",
    "\n",
    "    def K(self, X, X2=None, presliced=False):\n",
    "        if X2 is None:\n",
    "            return self.variance.get().expand(X.size(0), X.size(0))\n",
    "        else:\n",
    "            return self.variance.get().expand(X.size(0), X2.size(0))\n",
    "\n",
    "class Bias(Constant):\n",
    "    \"\"\"\n",
    "    Another name for the Constant kernel, included for convenience.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class Stationary(Kern):\n",
    "    \"\"\"\n",
    "    Base class for kernels that are stationary, that is, they only depend on\n",
    "\n",
    "        r = || x - x' ||\n",
    "\n",
    "    This class handles 'ARD' behaviour, which stands for 'Automatic Relevance\n",
    "    Determination'. This means that the kernel has one lengthscale per\n",
    "    dimension, otherwise the kernel is isotropic (has a single lengthscale).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, variance=1.0, lengthscales=None,\n",
    "                 active_dims=None, ARD=False, name=None):\n",
    "        \"\"\"\n",
    "        - input_dim is the dimension of the input to the kernel\n",
    "        - variance is the (initial) value for the variance parameter\n",
    "        - lengthscales is the initial value for the lengthscales parameter\n",
    "          defaults to 1.0 (ARD=False) or np.ones(input_dim) (ARD=True).\n",
    "        - active_dims is a list of length input_dim which controls which\n",
    "          columns of X are used.\n",
    "        - ARD specifies whether the kernel has one lengthscale per dimension\n",
    "          (ARD=True) or a single lengthscale (ARD=False).\n",
    "        \"\"\"\n",
    "        super(Stationary, self).__init__(input_dim, active_dims, name=name)\n",
    "        self.variance = parameter.PositiveParam(variance)\n",
    "        if ARD:\n",
    "            if lengthscales is None:\n",
    "                lengthscales = torch.ones(input_dim)\n",
    "            else:\n",
    "                # accepts float or array:\n",
    "                lengthscales = lengthscales * torch.ones(input_dim)\n",
    "            self.lengthscales = parameter.PositiveParam(lengthscales)\n",
    "            self.ARD = True\n",
    "        else:\n",
    "            if lengthscales is None:\n",
    "                lengthscales = 1.0\n",
    "            self.lengthscales = parameter.PositiveParam(lengthscales)\n",
    "            self.ARD = False\n",
    "\n",
    "    def square_dist(self, X, X2):\n",
    "        X = X / self.lengthscales.get()\n",
    "        Xs = (X**2).sum(1)\n",
    "\n",
    "        if X2 is None:\n",
    "            dist = -2 * torch.matmul(X, X.t())\n",
    "            dist += Xs.view(-1, 1) + Xs.view(1, -1)\n",
    "            return dist\n",
    "\n",
    "        X2 = X2 / self.lengthscales.get()\n",
    "        X2s = (X2**2).sum(1)\n",
    "        dist = -2 * torch.matmul(X, X2.t())\n",
    "        dist += Xs.view(-1, 1) + X2s.view(1, -1)\n",
    "        return dist\n",
    "\n",
    "\n",
    "    def euclid_dist(self, X, X2):\n",
    "        r2 = self.square_dist(X, X2)\n",
    "        return (r2 + 1e-12)**0.5\n",
    "\n",
    "    def Kdiag(self, X, presliced=False):\n",
    "        return self.variance.get().expand(X.size(0))\n",
    "\n",
    "class RBF(Stationary):\n",
    "    \"\"\"\n",
    "    The radial basis function (RBF) or squared exponential kernel\n",
    "    \"\"\"\n",
    "\n",
    "    def K(self, X, X2=None, presliced=False):\n",
    "        if not presliced:\n",
    "            X, X2 = self._slice(X, X2)\n",
    "        res = self.variance.get() * torch.exp(-0.5 * self.square_dist(X, X2))\n",
    "        return res\n",
    "\n",
    "class Exponential(Stationary):\n",
    "    \"\"\"\n",
    "    The Exponential kernel\n",
    "    \"\"\"\n",
    "\n",
    "    def K(self, X, X2=None, presliced=False):\n",
    "        if not presliced:\n",
    "            X, X2 = self._slice(X, X2)\n",
    "        r = self.euclid_dist(X, X2)\n",
    "        return self.variance.get() * torch.exp(-0.5 * r)\n",
    "\n",
    "\n",
    "class Matern12(Stationary):\n",
    "    \"\"\"\n",
    "    The Matern 1/2 kernel\n",
    "    \"\"\"\n",
    "\n",
    "    def K(self, X, X2=None, presliced=False):\n",
    "        if not presliced:\n",
    "            X, X2 = self._slice(X, X2)\n",
    "        r = self.euclid_dist(X, X2)\n",
    "        return self.variance.get() * torch.exp(-r)\n",
    "\n",
    "\n",
    "class Matern32(Stationary):\n",
    "    \"\"\"\n",
    "    The Matern 3/2 kernel\n",
    "    \"\"\"\n",
    "\n",
    "    def K(self, X, X2=None, presliced=False):\n",
    "        if not presliced:\n",
    "            X, X2 = self._slice(X, X2)\n",
    "        r = self.euclid_dist(X, X2)\n",
    "        return self.variance.get() * (1. + (3.**0.5) * r) * torch.exp(-(3.**0.5) * r)\n",
    "\n",
    "\n",
    "class Matern52(Stationary):\n",
    "    \"\"\"\n",
    "    The Matern 5/2 kernel\n",
    "    \"\"\"\n",
    "\n",
    "    def K(self, X, X2=None, presliced=False):\n",
    "        if not presliced:\n",
    "            X, X2 = self._slice(X, X2)\n",
    "        r = self.euclid_dist(X, X2)\n",
    "        return self.variance.get() * (1.0 + (5.**0.5) * r + 5. / 3. * r**2) * torch.exp(-(5.**0.5) * r)\n",
    "\n",
    "\n",
    "class Cosine(Stationary):\n",
    "    \"\"\"\n",
    "    The Cosine kernel\n",
    "    \"\"\"\n",
    "\n",
    "    def K(self, X, X2=None, presliced=False):\n",
    "        if not presliced:\n",
    "            X, X2 = self._slice(X, X2)\n",
    "        r = self.euclid_dist(X, X2)\n",
    "        return self.variance.get() * torch.cos(r)\n",
    "\n",
    "\n",
    "class ArcCosine(Kern):\n",
    "    \"\"\"\n",
    "    The Arc-cosine family of kernels which mimics the computation in neural\n",
    "    networks. The order parameter specifies the assumed activation function.\n",
    "    The Multi Layer Perceptron (MLP) kernel is closely related to the ArcCosine\n",
    "    kernel of order 0. The key reference is\n",
    "\n",
    "    ::\n",
    "\n",
    "        @incollection{NIPS2009_3628,\n",
    "            title = {Kernel Methods for Deep Learning},\n",
    "            author = {Youngmin Cho and Lawrence K. Saul},\n",
    "            booktitle = {Advances in Neural Information Processing Systems 22},\n",
    "            year = {2009},\n",
    "            url = {http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf}\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    implemented_orders = {0, 1, 2}\n",
    "    def __init__(self, input_dim,\n",
    "                 order=0,\n",
    "                 variance=1.0, weight_variances=1., bias_variance=1.,\n",
    "                 active_dims=None, ARD=False, name=None):\n",
    "        \"\"\"\n",
    "        - input_dim is the dimension of the input to the kernel\n",
    "        - order specifies the activation function of the neural network\n",
    "          the function is a rectified monomial of the chosen order.\n",
    "        - variance is the initial value for the variance parameter\n",
    "        - weight_variances is the initial value for the weight_variances parameter\n",
    "          defaults to 1.0 (ARD=False) or np.ones(input_dim) (ARD=True).\n",
    "        - bias_variance is the initial value for the bias_variance parameter\n",
    "          defaults to 1.0.\n",
    "        - active_dims is a list of length input_dim which controls which\n",
    "          columns of X are used.\n",
    "        - ARD specifies whether the kernel has one weight_variance per dimension\n",
    "          (ARD=True) or a single weight_variance (ARD=False).\n",
    "        \"\"\"\n",
    "        super(ArcCosine, self).__init__(input_dim, active_dims, name=name)\n",
    "\n",
    "        if order not in self.implemented_orders:\n",
    "            raise ValueError('Requested kernel order is not implemented.')\n",
    "        self.order = order\n",
    "\n",
    "        self.variance = parameter.PositiveParam(variance)\n",
    "        self.bias_variance = parameter.PositiveParam(variance)\n",
    "        self.ARD = ARD\n",
    "        if ARD:\n",
    "            if weight_variances is None:\n",
    "                weight_variances = self.variance.data.new(input_dim).fill_(1.0)\n",
    "            else:\n",
    "                # accepts float or Tensor:\n",
    "                weight_variances = weight_variances * self.variance.data.new(input_dim).fill_(1.0)\n",
    "            self.weight_variances = parameter.PositiveParam(weight_variances)\n",
    "        else:\n",
    "            if weight_variances is None:\n",
    "                weight_variances = 1.0\n",
    "            self.weight_variances = parameter.PositiveParam(weight_variances)\n",
    "\n",
    "    def _weighted_product(self, X, X2=None):\n",
    "        if X2 is None:\n",
    "            return (self.weight_variances.get() * (X**2)).sum(1) + self.bias_variance.get()\n",
    "        return torch.matmul(self.weight_variances.get() * X, X2.t()) + self.bias_variance.get()\n",
    "\n",
    "    def _J(self, theta):\n",
    "        \"\"\"\n",
    "        Implements the order dependent family of functions defined in equations\n",
    "        4 to 7 in the reference paper.\n",
    "        \"\"\"\n",
    "        if self.order == 0:\n",
    "            return float(numpy.pi) - theta\n",
    "        elif self.order == 1:\n",
    "            return torch.sin(theta) + (float(numpy.pi) - theta) * torch.cos(theta)\n",
    "        elif self.order == 2:\n",
    "            return 3. * torch.sin(theta) * torch.cos(theta) + (float(numpy.pi) - theta) * (1. + 2. * torch.cos(theta) ** 2)\n",
    "\n",
    "    def K(self, X, X2=None, presliced=False):\n",
    "        if not presliced:\n",
    "            X, X2 = self._slice(X, X2)\n",
    "\n",
    "        X_denominator = self._weighted_product(X)**0.5\n",
    "        if X2 is None:\n",
    "            X2 = X\n",
    "            X2_denominator = X_denominator\n",
    "        else:\n",
    "            X2_denominator = self._weighted_product(X2)**0.5\n",
    "\n",
    "        numerator = self._weighted_product(X, X2)\n",
    "        cos_theta = numerator / X_denominator[:, None] / X2_denominator[None, :]\n",
    "        jitter = 1e-15\n",
    "        theta = torch.acos(jitter + (1 - 2 * jitter) * cos_theta)\n",
    "\n",
    "        return ( self.variance.get() * (1. / float(numpy.pi)) * self._J(theta)\n",
    "                *X_denominator[:, None] ** self.order\n",
    "                *X2_denominator[None, :] ** self.order)\n",
    "\n",
    "    def Kdiag(self, X, presliced=False):\n",
    "        if not presliced:\n",
    "            X, _ = self._slice(X, None)\n",
    "\n",
    "        X_product = self._weighted_product(X)\n",
    "        theta = 0\n",
    "        return self.variance.get() * (1. / float(numpy.pi)) * self._J(theta) * X_product ** self.order\n",
    "\n",
    "\n",
    "class Combination(Kern):\n",
    "    \"\"\"\n",
    "    Combine  a list of kernels, e.g. by adding or multiplying (see inheriting\n",
    "    classes).\n",
    "\n",
    "    The names of the kernels to be combined are generated from their class\n",
    "    names.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kern_list, name=None):\n",
    "        for k in kern_list:\n",
    "            assert isinstance(k, Kern), \"can only add/multiply Kern instances\"\n",
    "\n",
    "        input_dim = numpy.max([k.input_dim if type(k.active_dims) is slice else numpy.max(k.active_dims) + 1 for k in kern_list])\n",
    "        super(Combination, self).__init__(input_dim=input_dim, name=name)\n",
    "\n",
    "        # add kernels to a list, flattening out instances of this class therein\n",
    "        self.kern_list = torch.nn.ModuleList()\n",
    "        for k in kern_list:\n",
    "            if isinstance(k, self.__class__):\n",
    "                self.kern_list.extend(k.kern_list)\n",
    "            else:\n",
    "                self.kern_list.append(k)\n",
    "\n",
    "    @property\n",
    "    def on_separate_dimensions(self):\n",
    "        \"\"\"\n",
    "        Checks whether the kernels in the combination act on disjoint subsets\n",
    "        of dimensions. Currently, it is hard to asses whether two slice objects\n",
    "        will overlap, so this will always return False.\n",
    "        :return: Boolean indicator.\n",
    "        \"\"\"\n",
    "        if numpy.any([isinstance(k.active_dims, slice) for k in self.kern_list]):\n",
    "            # Be conservative in the case of a slice object\n",
    "            return False\n",
    "        else:\n",
    "            dimlist = [k.active_dims for k in self.kern_list]\n",
    "            overlapping = False\n",
    "            for i, dims_i in enumerate(dimlist):\n",
    "                for dims_j in dimlist[i + 1:]:\n",
    "                    if numpy.any(dims_i.reshape(-1, 1) == dims_j.reshape(1, -1)):\n",
    "                        overlapping = True\n",
    "            return not overlapping\n",
    "\n",
    "        \n",
    "class Add(Combination):\n",
    "    def K(self, X, X2=None, presliced=False):\n",
    "        res = 0.0\n",
    "        for k in self.kern_list:\n",
    "            res += k.K(X, X2, presliced=presliced)\n",
    "        return res\n",
    "\n",
    "    def Kdiag(self, X, presliced=False):\n",
    "        res = 0.0\n",
    "        for k in self.kern_list:\n",
    "            res += k.Kdiag(X, presliced=presliced)\n",
    "        return res\n",
    "\n",
    "\n",
    "class Prod(Combination):\n",
    "    def K(self, X, X2=None, presliced=False):\n",
    "        res = 1.0\n",
    "        for k in self.kern_list:\n",
    "            res *= k.K(X, X2, presliced=presliced)\n",
    "        return res\n",
    "\n",
    "    def Kdiag(self, X, presliced=False):\n",
    "        res = 1.0\n",
    "        for k in self.kern_list:\n",
    "            res *= k.Kdiag(X, presliced=presliced)\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def func(x):\n",
    "    return torch.sin(x * 3*3.14) + 0.3*torch.cos(x * 9*3.14) + 0.5 * torch.sin(x * 7*3.14)\n",
    "\n",
    "\n",
    "X = torch.rand(10000, 1).double() * 2 - 1\n",
    "Y = func(X) + torch.randn(10000, 1).double() * 0.2\n",
    "pyplot.plot(X.numpy(), Y.numpy(), 'x')\n",
    "D = X.size(1)\n",
    "Xt = torch.linspace(-1.1, 1.1, 100).double().unsqueeze(1)\n",
    "Yt = func(Xt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/progyan.das/models/candleGP.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.0.62.168/home/progyan.das/models/candleGP.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.0.62.168/home/progyan.das/models/candleGP.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.0.62.168/home/progyan.das/models/candleGP.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m parameter\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.0.62.168/home/progyan.das/models/candleGP.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m quadrature\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.0.62.168/home/progyan.das/models/candleGP.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m densities\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "# Copyright 2016 Valentine Svensson, James Hensman, alexggmatthews, Alexis Boukouvalas\n",
    "# Copyright 2017 Artem Artemev @awav\n",
    "# Copyright 2017 Thomas Viehmann\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "from . import parameter\n",
    "from . import quadrature\n",
    "from . import densities\n",
    "\n",
    "class Likelihood(torch.nn.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.num_gauss_hermite_points = 20\n",
    "\n",
    "    def predict_mean_and_var(self, Fmu, Fvar):\n",
    "        \"\"\"\n",
    "        Given a Normal distribution for the latent function,\n",
    "        return the mean of Y\n",
    "\n",
    "        if\n",
    "            q(f) = N(Fmu, Fvar)\n",
    "\n",
    "        and this object represents\n",
    "\n",
    "            p(y|f)\n",
    "\n",
    "        then this method computes the predictive mean\n",
    "\n",
    "           \\int\\int y p(y|f)q(f) df dy\n",
    "\n",
    "        and the predictive variance\n",
    "\n",
    "           \\int\\int y^2 p(y|f)q(f) df dy  - [ \\int\\int y^2 p(y|f)q(f) df dy ]^2\n",
    "\n",
    "        Here, we implement a default Gauss-Hermite quadrature routine, but some\n",
    "        likelihoods (e.g. Gaussian) will implement specific cases.\n",
    "        \"\"\"\n",
    "        gh_x, gh_w = hermgauss(self.num_gauss_hermite_points)\n",
    "        gh_w /= float(numpy.pi**0.5)\n",
    "        gh_w = gh_w.reshape(-1, 1)\n",
    "        shape = Fmu.size()\n",
    "        Fmu = Fmu.view(-1,1)\n",
    "        Fvar = Fvar.view(-1,1)\n",
    "        X = gh_x[None, :] * (2.0 * Fvar)**0.5 + Fmu\n",
    "\n",
    "        # here's the quadrature for the mean\n",
    "        E_y = torch.matmul(self.conditional_mean(X), gh_w).view(shape)\n",
    "\n",
    "        # here's the quadrature for the variance\n",
    "        integrand = self.conditional_variance(X) + (self.conditional_mean(X))**2\n",
    "        V_y = torch.matmul(integrand, gh_w).view(shape) - E_y**2\n",
    "\n",
    "        return E_y, V_y\n",
    "\n",
    "    def predict_density(self, Fmu, Fvar, Y):\n",
    "        \"\"\"\n",
    "        Given a Normal distribution for the latent function, and a datum Y,\n",
    "        compute the (log) predictive density of Y.\n",
    "\n",
    "        i.e. if\n",
    "            q(f) = N(Fmu, Fvar)\n",
    "\n",
    "        and this object represents\n",
    "\n",
    "            p(y|f)\n",
    "\n",
    "        then this method computes the predictive density\n",
    "\n",
    "           \\int p(y=Y|f)q(f) df\n",
    "\n",
    "        Here, we implement a default Gauss-Hermite quadrature routine, but some\n",
    "        likelihoods (Gaussian, Poisson) will implement specific cases.\n",
    "        \"\"\"\n",
    "        gh_x, gh_w = quadrature.hermgauss(self.num_gauss_hermite_points, dtype=Fmu.dtype)\n",
    "\n",
    "        gh_w = gh_w.reshape(-1, 1) / float(numpy.sqrt(numpy.pi))\n",
    "        shape = Fmu.size()\n",
    "        Fmu, Fvar, Y = [e.view(-1, 1) for e in (Fmu, Fvar, Y)]\n",
    "        X = gh_x * (2.0 * Fvar)**0.5 + Fmu\n",
    "        Y = Y.expand(-1, self.num_gauss_hermite_points)  # broadcast Y to match X\n",
    "        logp = self.logp(X, Y)\n",
    "        return torch.matmul(logp.exp(), gh_w).view(*shape)\n",
    "\n",
    "    def variational_expectations(self, Fmu, Fvar, Y):\n",
    "        \"\"\"\n",
    "        Compute the expected log density of the data, given a Gaussian\n",
    "        distribution for the function values.\n",
    "\n",
    "        if\n",
    "            q(f) = N(Fmu, Fvar)\n",
    "\n",
    "        and this object represents\n",
    "\n",
    "            p(y|f)\n",
    "\n",
    "        then this method computes\n",
    "\n",
    "           \\int (\\log p(y|f)) q(f) df.\n",
    "\n",
    "\n",
    "        Here, we implement a default Gauss-Hermite quadrature routine, but some\n",
    "        likelihoods (Gaussian, Poisson) will implement specific cases.\n",
    "        \"\"\"\n",
    "\n",
    "        gh_x, gh_w = quadrature.hermgauss(self.num_gauss_hermite_points, dtype=Fmu.dtype)\n",
    "        gh_x = gh_x.view(1, -1)\n",
    "        gh_w = gh_w.view(-1, 1) / float(numpy.pi)**0.5\n",
    "        shape = Fmu.size()\n",
    "        Fmu, Fvar, Y = [e.view(-1, 1) for e in (Fmu, Fvar, Y)]\n",
    "        X = gh_x * (2.0 * Fvar)**0.5 + Fmu\n",
    "        Y = Y.expand(-1, self.num_gauss_hermite_points)  # broadcast Y to match X\n",
    "        logp = self.logp(X, Y)\n",
    "        return torch.matmul(logp, gh_w).view(*shape)\n",
    "\n",
    "    def _check_targets(self, Y_np):  # pylint: disable=R0201\n",
    "        \"\"\"\n",
    "        Check that the Y values are valid for the likelihood.\n",
    "        Y_np is a numpy array.\n",
    "\n",
    "        The base class check is that the array has two dimensions\n",
    "        and consists only of floats. The float requirement is so that AutoFlow\n",
    "        can work with Model.predict_density.\n",
    "        \"\"\"\n",
    "        if not Y.dim() == 2:\n",
    "            raise ValueError('targets must be shape N x D')\n",
    "        #if np.array(list(Y_np)).dtype != settings.np_float:\n",
    "        #    raise ValueError('use {}, even for discrete variables'.format(settings.np_float))\n",
    "\n",
    "\n",
    "class Gaussian(Likelihood):\n",
    "    def __init__(self, dtype=torch.float32):\n",
    "        Likelihood.__init__(self)\n",
    "        self.variance = parameter.PositiveParam(torch.tensor([1.0], dtype=dtype), dtype=dtype)\n",
    "\n",
    "    def logp(self, F, Y):\n",
    "        return densities.gaussian(F, Y, self.variance.get())\n",
    "\n",
    "    def conditional_mean(self, F):\n",
    "        return F\n",
    "\n",
    "    def conditional_variance(self, F):\n",
    "        return self.variance.get().expand_as(F)\n",
    "\n",
    "    def predict_mean_and_var(self, Fmu, Fvar):\n",
    "        return Fmu, Fvar + self.variance.get()\n",
    "\n",
    "    def predict_density(self, Fmu, Fvar, Y):\n",
    "        return densities.gaussian(Fmu, Y, Fvar + self.variance.get())\n",
    "\n",
    "    def variational_expectations(self, Fmu, Fvar, Y):\n",
    "        return (-0.5 * numpy.log(2 * numpy.pi) - 0.5 * torch.log(self.variance.get())\n",
    "                - 0.5 * ((Y - Fmu)**2 + Fvar) / self.variance.get())\n",
    "\n",
    "\n",
    "def probit(x):\n",
    "    return 0.5 * (1.0 + torch.erf(x / (2.0**0.5))) * (1 - 2e-3) + 1e-3\n",
    "\n",
    "\n",
    "class Bernoulli(Likelihood):\n",
    "    def __init__(self, invlink=probit):\n",
    "        super(Bernoulli, self).__init__()\n",
    "        self.invlink = invlink\n",
    "\n",
    "    def _check_targets(self, Y_np):\n",
    "        super(Bernoulli, self)._check_targets(Y_np)\n",
    "        Y_set = set(Y_np.flatten())\n",
    "        if len(Y_set) > 2 or len(Y_set - set([1.])) > 1:\n",
    "            raise Warning('all bernoulli variables should be in {1., k}, for some k')\n",
    "\n",
    "    def logp(self, F, Y):\n",
    "        return densities.bernoulli(self.invlink(F), Y)\n",
    "\n",
    "    def predict_mean_and_var(self, Fmu, Fvar):\n",
    "        if self.invlink is probit:\n",
    "            p = probit(Fmu / (1 + Fvar)**0.5)\n",
    "            return p, p - p**2\n",
    "        else:\n",
    "            # for other invlink, use quadrature\n",
    "            return Likelihood.predict_mean_and_var(self, Fmu, Fvar)\n",
    "\n",
    "    def predict_density(self, Fmu, Fvar, Y):\n",
    "        p = self.predict_mean_and_var(Fmu, Fvar)[0]\n",
    "        return densities.bernoulli(p, Y)\n",
    "\n",
    "    def conditional_mean(self, F):\n",
    "        return self.invlink(F)\n",
    "\n",
    "    def conditional_variance(self, F):\n",
    "        p = self.invlink(F)\n",
    "        return p - p**2\n",
    "\n",
    "    \n",
    "class Exponential(Likelihood):\n",
    "    def __init__(self, invlink=torch.exp):\n",
    "        Likelihood.__init__(self)\n",
    "        self.invlink = invlink\n",
    "\n",
    "    def _check_targets(self, Y):\n",
    "        super(Exponential, self)._check_targets(Y)\n",
    "        if (Y < 0).any():\n",
    "            raise ValueError('exponential variables must be positive')\n",
    "\n",
    "    def logp(self, F, Y):\n",
    "        return densities.exponential(self.invlink(F), Y)\n",
    "\n",
    "    def conditional_mean(self, F):\n",
    "        return self.invlink(F)\n",
    "\n",
    "    def conditional_variance(self, F):\n",
    "        return (self.invlink(F))**2\n",
    "\n",
    "    def variational_expectations(self, Fmu, Fvar, Y):\n",
    "        if self.invlink is torch.exp:\n",
    "            return - torch.exp(-Fmu + Fvar / 2) * Y - Fmu\n",
    "        return super(Exponential, self).variational_expectations(Fmu, Fvar, Y)\n",
    "\n",
    "class RobustMax(object):\n",
    "    \"\"\"\n",
    "    This class represent a multi-class inverse-link function. Given a vector\n",
    "    f=[f_1, f_2, ... f_k], the result of the mapping is\n",
    "\n",
    "    y = [y_1 ... y_k]\n",
    "\n",
    "    with\n",
    "\n",
    "    y_i = (1-eps)  i == argmax(f)\n",
    "          eps/(k-1)  otherwise.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes, epsilon=1e-3):\n",
    "        self.epsilon = epsilon\n",
    "        self.num_classes = num_classes\n",
    "        self._eps_K1 = self.epsilon / (self.num_classes - 1.)\n",
    "\n",
    "    def __call__(self, F):\n",
    "        _,i = torch.max(F.data, 1)\n",
    "        one_hot = torch.full((F.size(0), self.num_classes), self._eps_K1, dtype=F.dtype, device=F.device).scatter_(1, i, 1 - self.epsilon)\n",
    "        return one_hot\n",
    "\n",
    "    def prob_is_largest(self, Y, mu, var, gh_x, gh_w):\n",
    "        Y = Y.long()\n",
    "        # work out what the mean and variance is of the indicated latent function.\n",
    "        oh_on = torch.zeros(Y.numel(), self.num_classes, dtype=mu.dtype, device=mu.device).scatter_(1, Y.data, 1)\n",
    "        mu_selected  = (oh_on * mu ).sum(1)\n",
    "        var_selected = (oh_on * var).sum(1)\n",
    "\n",
    "        # generate Gauss Hermite grid\n",
    "        X = mu_selected.view(-1, 1) + gh_x * ((2. * var_selected).clamp(min=1e-10)**0.5).view(-1,1)\n",
    "\n",
    "        # compute the CDF of the Gaussian between the latent functions and the grid (including the selected function)\n",
    "        dist = (X.unsqueeze(1) - mu.unsqueeze(2)) / (var.clamp(min=1e-10)**0.5).unsqueeze(2)\n",
    "        cdfs = 0.5 * (1.0 + torch.erf(dist / 2.0**0.5))\n",
    "\n",
    "        cdfs = cdfs * (1 - 2e-4) + 1e-4\n",
    "\n",
    "        # blank out all the distances on the selected latent function\n",
    "        oh_off = torch.ones(Y.numel(), self.num_classes, dtype=mu.dtype, device=mu.device).scatter_(1,Y.data,0)\n",
    "        cdfs = cdfs * oh_off.unsqueeze(2) + oh_on.unsqueeze(2)\n",
    "\n",
    "        # take the product over the latent functions, and the sum over the GH grid.\n",
    "        return torch.matmul(cdfs.prod(1), gh_w.view(-1,1) / (numpy.pi**0.5))\n",
    "\n",
    "\n",
    "class MultiClass(Likelihood):\n",
    "    def __init__(self, num_classes, invlink=None):\n",
    "        \"\"\"\n",
    "        A likelihood that can do multi-way classification.\n",
    "        Currently the only valid choice\n",
    "        of inverse-link function (invlink) is an instance of RobustMax.\n",
    "        \"\"\"\n",
    "        super(MultiClass, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        if invlink is None:\n",
    "            invlink = RobustMax(self.num_classes)\n",
    "        elif not isinstance(invlink, RobustMax):\n",
    "            raise NotImplementedError(\"Multiclass currently only supports RobustMax link\")\n",
    "        self.invlink = invlink\n",
    "\n",
    "    def _check_targets(self, Y_np):\n",
    "        super(MultiClass, self)._check_targets(Y_np)\n",
    "        if not set(Y_np.view(-1)).issubset(set(range(self.num_classes))):\n",
    "            raise ValueError('multiclass likelihood expects inputs to be in {0., 1., 2.,...,k-1}')\n",
    "        if Y_np.size(1) != 1:\n",
    "            raise ValueError('only one dimension currently supported for multiclass likelihood')\n",
    "\n",
    "    def logp(self, F, Y):\n",
    "        if isinstance(self.invlink, RobustMax):\n",
    "            p = (torch.max(F, 1)[1].unsqueeze(1)==Y.long())*(1-self.invlink.epsilon-self.invlink._eps_K1)+self.invlink._eps_K1\n",
    "            return torch.log(p)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Multiclass currently only supports RobustMax link\")\n",
    "\n",
    "    def variational_expectations(self, Fmu, Fvar, Y):\n",
    "        if isinstance(self.invlink, RobustMax):\n",
    "            gh_x, gh_w = quadrature.hermgauss(self.num_gauss_hermite_points, dtype=Fmu.dtype)\n",
    "            p = self.invlink.prob_is_largest(Y, Fmu, Fvar, gh_x, gh_w)\n",
    "            return p * numpy.log(1 - self.invlink.epsilon) + (1. - p) * numpy.log(self.invlink._eps_K1)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Multiclass currently only supports RobustMax link\")\n",
    "\n",
    "    def predict_mean_and_var(self, Fmu, Fvar):\n",
    "        if isinstance(self.invlink, RobustMax):\n",
    "            # To compute this, we'll compute the density for each possible output\n",
    "            possible_outputs = [torch.full((Fmu.size(0), 1), i, dtype=torch.long, device=Fmu.device) for i in range(self.num_classes)]\n",
    "            ps = [self._predict_non_logged_density(Fmu, Fvar, po) for po in possible_outputs]\n",
    "            ps = torch.stack([p.view(-1) for p in ps],1)\n",
    "            return ps, ps - ps**2\n",
    "        else:\n",
    "            raise NotImplementedError(\"Multiclass currently only supports RobustMax link\")\n",
    "\n",
    "    def predict_density(self, Fmu, Fvar, Y):\n",
    "        return torch.log(self._predict_non_logged_density(Fmu, Fvar, Y))\n",
    "\n",
    "    def _predict_non_logged_density(self, Fmu, Fvar, Y):\n",
    "        if isinstance(self.invlink, RobustMax):\n",
    "            gh_x, gh_w = quadrature.hermgauss(self.num_gauss_hermite_points, dtype=Fmu.dtype)\n",
    "            p = self.invlink.prob_is_largest(Y, Fmu, Fvar, gh_x, gh_w)\n",
    "            return p * (1 - self.invlink.epsilon) + (1. - p) * (self.invlink._eps_K1)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Multiclass currently only supports RobustMax link\")\n",
    "\n",
    "    def conditional_mean(self, F):\n",
    "        return self.invlink(F)\n",
    "\n",
    "    def conditional_variance(self, F):\n",
    "        p = self.conditional_mean(F)\n",
    "        return p - p**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = RBF(D,variance=torch.DoubleTensor([1.0])).double()\n",
    "Z = X[:M].clone()\n",
    "m = SVGP(Variable(X), Variable(Y.unsqueeze(1)),\n",
    "                         likelihood=Gaussian(ttype=torch.DoubleTensor),\n",
    "                         kern=k, Z=Z)\n",
    "m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9293687d4483a4b3a1700f8c01352879b95dd58a76d6ca7a0b1cbd6a5f4d62ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
